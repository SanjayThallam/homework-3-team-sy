{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with existing Sklearn, Pandas, and Numpy functions.\n",
    "\n",
    "In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "\n",
    "* Do some feature engineering\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We will use a dataset consisting of Amazon.com product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alexa Voice Remote for Amazon Echo and Echo Dot',\n",
       "       'Alexa Voice Remote for Amazon Fire TV and Fire TV Stick',\n",
       "       'All-New Amazon Fire 7 Tablet Case (7th Generation',\n",
       "       'All-New Amazon Fire HD 8 Tablet Case (7th Generation',\n",
       "       'All-New Amazon Fire TV Game Controller',\n",
       "       'All-New Amazon Kid-Proof Case for Amazon Fire 7 Tablet (7th Generation',\n",
       "       'All-New Amazon Kid-Proof Case for Amazon Fire HD 8 Tablet (7th Generation',\n",
       "       'All-New Fire 7 Kids Edition Tablet',\n",
       "       'All-New Fire 7 Tablet with Alexa',\n",
       "       'All-New Fire HD 8 Kids Edition Tablet',\n",
       "       'All-New Fire HD 8 Tablet with Alexa',\n",
       "       'Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders',\n",
       "       'Amazon Echo - Black',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Charcoal Fabric',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Indigo Fabric',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Merlot Leather',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Midnight Leather',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Saddle Tan Leather',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Sandstone Fabric',\n",
       "       'Amazon Fire TV', 'Amazon Fire TV Game Controller',\n",
       "       'Amazon Kindle Oasis Premium Leather Battery Cover - Black',\n",
       "       'Amazon Kindle Oasis Premium Leather Battery Cover - Walnut',\n",
       "       'Amazon Premium Headphones',\n",
       "       'Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker',\n",
       "       'Amazon Tap Sling Cover - Blue', 'Amazon Tap Sling Cover - Green',\n",
       "       'Amazon Tap Sling Cover - Magenta',\n",
       "       'Amazon Tap Sling Cover - Tangerine',\n",
       "       'Amazon Tap Sling Cover - White',\n",
       "       'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)',\n",
       "       'Certified Refurbished Echo Dot (2nd Generation) - Black',\n",
       "       'Certified Refurbished Fire HD 10 Tablet',\n",
       "       'Certified Refurbished Kindle E-reader',\n",
       "       'Certified Refurbished Kindle E-reader - Black',\n",
       "       'Certified Refurbished Kindle Paperwhite E-reader - Black',\n",
       "       'Certified Refurbished Kindle Voyage E-reader with Special Offers',\n",
       "       'Echo Dot (2nd Generation) - Black', 'Echo Show - Black',\n",
       "       'Fire HD 10 Tablet with Alexa', 'Fire HD 6 Tablet',\n",
       "       'Fire HD 7 Tablet', 'Fire HD 8 Tablet',\n",
       "       'Fire HD 8 Tablet with Alexa', 'Fire HDX 8.9 Tablet',\n",
       "       'Fire Kids Edition Tablet', 'Fire Tablet with Alexa', 'Kindle',\n",
       "       'Kindle E-reader - Black', 'Kindle Fire HD 7\"',\n",
       "       'Kindle Fire HDX 7\"', 'Kindle Fire HDX 8.9\"', 'Kindle Keyboard',\n",
       "       'Kindle Oasis E-reader with Leather Charging Cover - Walnut',\n",
       "       'Kindle Oasis with Leather Charging Cover - Black',\n",
       "       'Kindle Paperwhite', 'Kindle Paperwhite 3G',\n",
       "       'Kindle Paperwhite E-reader - Black', 'Kindle Voyage E-reader',\n",
       "       'Kindle for Kids Bundle with the latest Kindle E-reader',\n",
       "       'Moshi Anti-Glare No Bubble Screen Protector for the Fire Phone',\n",
       "       'Replacement Remote for Amazon Fire TV Stick'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = pd.read_csv('../datasets/Amazon Product Reviews I.csv')\n",
    "np.unique(products['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see a preview of what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  rating  \\\n",
      "0  I initially had trouble deciding between the p...     5.0   \n",
      "1  Allow me to preface this with a little history...     5.0   \n",
      "2  I am enjoying it so far. Great for reading. Ha...     4.0   \n",
      "3  I bought one of the first Paperwhites and have...     5.0   \n",
      "4  I have to say upfront - I don't like coroporat...     5.0   \n",
      "\n",
      "                                        title  \n",
      "0              Paperwhite voyage, no regrets!  \n",
      "1           One Simply Could Not Ask For More  \n",
      "2  Great for those that just want an e-reader  \n",
      "3                    Love / Hate relationship  \n",
      "4                                   I LOVE IT  \n"
     ]
    }
   ],
   "source": [
    "# Simplify relevant columns names\n",
    "if('reviews.rating' in products.columns):\n",
    "    products['rating']=products['reviews.rating']\n",
    "    products.drop(['reviews.rating'],axis=1, inplace=True)\n",
    "\n",
    "if('reviews.text' in products.columns):\n",
    "    products['reviews']=products['reviews.text']\n",
    "    products.drop(['reviews.text'],axis=1, inplace=True)\n",
    "    \n",
    "if('reviews.title' in products.columns):\n",
    "    products['title']=products['reviews.title']\n",
    "    products.drop(['reviews.title'],axis=1, inplace=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "relevant_cols=['reviews','rating','title']\n",
    "products = products.loc[:, relevant_cols]\n",
    "\n",
    "# Drop Nana\n",
    "products.dropna(subset=['rating', 'reviews','title'], inplace=True)\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore a specific example of a Amazon product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can read a lot longer without pain in my hands since the cover holds it for me. Very nice shade of blue.\n"
     ]
    }
   ],
   "source": [
    "print(products['reviews'][269])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into word-counts.\n",
    "\n",
    "**Aside**. In this notebook, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth. See [this page](https://neptune.ai/blog/tokenization-in-nlp) for an example of smart handling of punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I initially had trouble deciding between the p...\n",
      "1       Allow me to preface this with a little history...\n",
      "2       I am enjoying it so far Great for reading Had ...\n",
      "3       I bought one of the first Paperwhites and have...\n",
      "4       I have to say upfront  I dont like coroporate ...\n",
      "                              ...                        \n",
      "1172    This is not the same remote that I got for my ...\n",
      "1173    I have had to change the batteries in this rem...\n",
      "1174    Remote did not activate nor did it connect to ...\n",
      "1175    It does the job but is super over priced I fee...\n",
      "1176    I ordered this item to replace the one that no...\n",
      "Name: reviews, Length: 1177, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "def remove_punctuation(text):\n",
    "    try: # python 2.x\n",
    "        text = text.translate(None, string.punctuation) \n",
    "        #tokens = word_tokenize(text)\n",
    "    except: # python 3.x\n",
    "        translator = text.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "        #tokens = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "products['reviews'] = products['reviews'].apply(remove_punctuation)\n",
    "print(products['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1177, 6750)\n",
      "(1177, 6750)\n",
      "                                             reviews  rating  \\\n",
      "0  I initially had trouble deciding between the p...     5.0   \n",
      "1  Allow me to preface this with a little history...     5.0   \n",
      "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
      "3  I bought one of the first Paperwhites and have...     5.0   \n",
      "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
      "\n",
      "                                        title  word_count_0  word_count_1  \\\n",
      "0              Paperwhite voyage, no regrets!           0.0           0.0   \n",
      "1           One Simply Could Not Ask For More           0.0           0.0   \n",
      "2  Great for those that just want an e-reader           0.0           0.0   \n",
      "3                    Love / Hate relationship           0.0           0.0   \n",
      "4                                   I LOVE IT           0.0           0.0   \n",
      "\n",
      "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
      "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "\n",
      "   word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   word_count_6748  word_count_6749  \n",
      "0              0.0              0.0  \n",
      "1              0.0              0.0  \n",
      "2              0.0              0.0  \n",
      "3              0.0              0.0  \n",
      "4              0.0              0.0  \n",
      "\n",
      "[5 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Frequency counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(products['reviews'])\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "word_count_df = pd.DataFrame(X_train_tfidf.toarray())\n",
    "word_count_df = word_count_df.add_prefix('word_count_')\n",
    "products = pd.concat([products, word_count_df], axis=1)\n",
    "\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations. Here, each entry in the **word_count** column is a dictionary where the key is the word and the value is a count of the number of times the word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 6753)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far Great for reading Had ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront  I dont like coroporate ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
       "\n",
       "                                        title  word_count_0  word_count_1  \\\n",
       "0              Paperwhite voyage, no regrets!           0.0           0.0   \n",
       "1           One Simply Could Not Ask For More           0.0           0.0   \n",
       "2  Great for those that just want an e-reader           0.0           0.0   \n",
       "3                    Love / Hate relationship           0.0           0.0   \n",
       "4                                   I LOVE IT           0.0           0.0   \n",
       "\n",
       "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "\n",
       "   word_count_6741  word_count_6742  word_count_6743  word_count_6744  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_count_6745  word_count_6746  word_count_6747  word_count_6748  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_count_6749  sentiment  \n",
       "0              0.0          1  \n",
       "1              0.0          1  \n",
       "2              0.0          1  \n",
       "3              0.0          1  \n",
       "4              0.0          1  \n",
       "\n",
       "[5 rows x 6754 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda r : +1 if r > 3 else -1)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'sentiment'}>]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View number of positive and negative sentiment\n",
    "products.hist(column=['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1).\n",
    "\n",
    "Note, there are significantly more positive reviews than negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match number of positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 977 positive reviews\n",
      "There are 76 negative reviews\n",
      "[Update] There are 76 positive reviews\n",
      "                                               reviews  rating  \\\n",
      "43   After 15 months my 500 tablet is no longer use...     1.0   \n",
      "69   This is a great controller for our Fire TV I p...     2.0   \n",
      "79   This is a great controller for our Fire TV I p...     2.0   \n",
      "84   The connection drops out on this unit pretty b...     1.0   \n",
      "117  As the Kids Edition is nothing more than a bas...     1.0   \n",
      "\n",
      "                                                 title  word_count_0  \\\n",
      "43                                Dead after 15 months           0.0   \n",
      "69   Great controller with 2 major problems! Not re...           0.0   \n",
      "79   Great controller with 2 major problems! Not re...           0.0   \n",
      "84   This needs to be way better for the amount I s...           0.0   \n",
      "117          Freetime makes me angry. So, so angry. 3,           0.0   \n",
      "\n",
      "     word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "43            0.0           0.0           0.0           0.0           0.0   \n",
      "69            0.0           0.0           0.0           0.0           0.0   \n",
      "79            0.0           0.0           0.0           0.0           0.0   \n",
      "84            0.0           0.0           0.0           0.0           0.0   \n",
      "117           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     word_count_6  ...  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "43            0.0  ...              0.0              0.0              0.0   \n",
      "69            0.0  ...              0.0              0.0              0.0   \n",
      "79            0.0  ...              0.0              0.0              0.0   \n",
      "84            0.0  ...              0.0              0.0              0.0   \n",
      "117           0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "43               0.0              0.0              0.0              0.0   \n",
      "69               0.0              0.0              0.0              0.0   \n",
      "79               0.0              0.0              0.0              0.0   \n",
      "84               0.0              0.0              0.0              0.0   \n",
      "117              0.0              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6748  word_count_6749  sentiment  \n",
      "43               0.0              0.0         -1  \n",
      "69               0.0              0.0         -1  \n",
      "79               0.0              0.0         -1  \n",
      "84               0.0              0.0         -1  \n",
      "117              0.0              0.0         -1  \n",
      "\n",
      "[5 rows x 6754 columns]\n",
      "                                               reviews  rating  \\\n",
      "704  Good price works great would buy again good va...     5.0   \n",
      "270  This is an improvement over the Origami case w...     5.0   \n",
      "574  Alexa speaking from the Amazon Tap is great to...     4.0   \n",
      "897  I really liked this tablet Got it for my nephe...     4.0   \n",
      "799  Good sound but unlike Echo you have to press a...     4.0   \n",
      "\n",
      "                                       title  word_count_0  word_count_1  \\\n",
      "704                              Great price           0.0           0.0   \n",
      "270  Very complimentary to the new Fire HD 8           0.0           0.0   \n",
      "574       Useful info and fun to have around           0.0           0.0   \n",
      "897               Has a lot of apps built in           0.0           0.0   \n",
      "799                Would be better with Dot.           0.0           0.0   \n",
      "\n",
      "     word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  \\\n",
      "704           0.0           0.0           0.0           0.0           0.0   \n",
      "270           0.0           0.0           0.0           0.0           0.0   \n",
      "574           0.0           0.0           0.0           0.0           0.0   \n",
      "897           0.0           0.0           0.0           0.0           0.0   \n",
      "799           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     ...  word_count_6741  word_count_6742  word_count_6743  word_count_6744  \\\n",
      "704  ...              0.0              0.0              0.0              0.0   \n",
      "270  ...              0.0              0.0              0.0              0.0   \n",
      "574  ...              0.0              0.0              0.0              0.0   \n",
      "897  ...              0.0              0.0              0.0              0.0   \n",
      "799  ...              0.0              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6745  word_count_6746  word_count_6747  word_count_6748  \\\n",
      "704              0.0              0.0              0.0              0.0   \n",
      "270              0.0              0.0              0.0              0.0   \n",
      "574              0.0              0.0              0.0              0.0   \n",
      "897              0.0              0.0              0.0              0.0   \n",
      "799              0.0              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6749  sentiment  \n",
      "704              0.0          1  \n",
      "270              0.0          1  \n",
      "574              0.0          1  \n",
      "897              0.0          1  \n",
      "799              0.0          1  \n",
      "\n",
      "[5 rows x 6754 columns]\n"
     ]
    }
   ],
   "source": [
    "# Report number of positive examples\n",
    "positive_sent = products[products['sentiment']==1]\n",
    "print('There are {} positive reviews'.format(len(positive_sent)))\n",
    "\n",
    "# Report number of negative examples\n",
    "negative_sent = products[products['sentiment']==-1]\n",
    "print('There are {} negative reviews'.format(len(negative_sent)))\n",
    "\n",
    "# Sample number of negative example from positive examples (# positive > # negative)\n",
    "positive_sample = positive_sent.sample(n = len(negative_sent))\n",
    "print('[Update] There are {} positive reviews'.format(len(positive_sample)))\n",
    "\n",
    "# Merge positive and negative examples and update products dataframe\n",
    "frames = [negative_sent, positive_sample]\n",
    "products = pd.concat(frames)\n",
    "print(products.head())\n",
    "print(products.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = products.loc[:, ~products.columns.isin(['sentiment'])], products.loc[:, products.columns.isin(['sentiment'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count** as a feature and the column **sentiment** as the target. We will use `validation_set=None` to obtain same results as everyone else.\n",
    "\n",
    "**Note:** This line may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>word_count_7</th>\n",
       "      <th>word_count_8</th>\n",
       "      <th>word_count_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_count_0  word_count_1  word_count_2  word_count_3  word_count_4  \\\n",
       "229           0.0           0.0           0.0           0.0           0.0   \n",
       "912           0.0           0.0           0.0           0.0           0.0   \n",
       "837           0.0           0.0           0.0           0.0           0.0   \n",
       "728           0.0           0.0           0.0           0.0           0.0   \n",
       "211           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     word_count_5  word_count_6  word_count_7  word_count_8  word_count_9  \\\n",
       "229           0.0           0.0           0.0           0.0           0.0   \n",
       "912           0.0           0.0           0.0           0.0           0.0   \n",
       "837           0.0           0.0           0.0           0.0           0.0   \n",
       "728           0.0           0.0           0.0           0.0           0.0   \n",
       "211           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     ...  word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
       "229  ...              0.0              0.0              0.0              0.0   \n",
       "912  ...              0.0              0.0              0.0              0.0   \n",
       "837  ...              0.0              0.0              0.0              0.0   \n",
       "728  ...              0.0              0.0              0.0              0.0   \n",
       "211  ...              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
       "229              0.0              0.0              0.0              0.0   \n",
       "912              0.0              0.0              0.0              0.0   \n",
       "837              0.0              0.0              0.0              0.0   \n",
       "728              0.0              0.0              0.0              0.0   \n",
       "211              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6748  word_count_6749  \n",
       "229              0.0              0.0  \n",
       "912              0.0              0.0  \n",
       "837              0.0              0.0  \n",
       "728              0.0              0.0  \n",
       "211              0.0              0.0  \n",
       "\n",
       "[5 rows x 6750 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sentiment = X_train.loc[:,X_train.columns.str.startswith('word_count_')]\n",
    "X_test_sentiment = X_test.loc[:,X_test.columns.str.startswith('word_count_')]\n",
    "\n",
    "X_train_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sentiment_model = LogisticRegression(random_state=0)\n",
    "sentiment_model.fit(X_train_sentiment, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. You may get a warning to the effect of \"Terminated due to numerical difficulties --- this model may not be ideal\". It means that the quality metric (to be covered in Module 3) failed to improve in the last iteration of the run. The difficulty arises as the sentiment model puts too much weight on extremely rare words. A way to rectify this is to apply regularization, to be covered in Module 4. Regularization lessens the effect of extremely rare words. For the purpose of this assignment, however, please proceed with the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "weights = sentiment_model.coef_\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of `6750` coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. \n",
    "\n",
    "Fill in the following block of code to calculate how many *weights* are positive ( >= 0). (**Hint**: Use numpy to sum the weights and check *weights* must be positive ( >= 0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive weights: 5674 \n",
      "Number of negative weights: 1076 \n"
     ]
    }
   ],
   "source": [
    "num_positive_weights = np.sum(weights >= 0)\n",
    "num_negative_weights = np.sum(weights < 0)\n",
    "\n",
    "print(\"Number of positive weights: %s \" % num_positive_weights)\n",
    "print(\"Number of negative weights: %s \" % num_negative_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** How many weights are >= 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 3 examples in the test dataset.  We refer to this set of 3 examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015    5.0\n",
      "983     5.0\n",
      "821     5.0\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>I bought these for a couple of reasonsFirst I ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I hate having to shove headphones into my brai...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>Excellent product awesome color</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great color One person found this helpful. Was...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>This is a great piece of hardwareI love it rea...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lovely speaker with many extras</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  rating  \\\n",
       "1015  I bought these for a couple of reasonsFirst I ...     5.0   \n",
       "983                     Excellent product awesome color     5.0   \n",
       "821   This is a great piece of hardwareI love it rea...     5.0   \n",
       "\n",
       "                                                  title  word_count_0  \\\n",
       "1015  I hate having to shove headphones into my brai...           0.0   \n",
       "983   Great color One person found this helpful. Was...           0.0   \n",
       "821                     lovely speaker with many extras           0.0   \n",
       "\n",
       "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
       "1015           0.0           0.0           0.0           0.0           0.0   \n",
       "983            0.0           0.0           0.0           0.0           0.0   \n",
       "821            0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
       "1015           0.0  ...              0.0              0.0              0.0   \n",
       "983            0.0  ...              0.0              0.0              0.0   \n",
       "821            0.0  ...              0.0              0.0              0.0   \n",
       "\n",
       "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
       "1015              0.0              0.0              0.0              0.0   \n",
       "983               0.0              0.0              0.0              0.0   \n",
       "821               0.0              0.0              0.0              0.0   \n",
       "\n",
       "      word_count_6747  word_count_6748  word_count_6749  \n",
       "1015              0.0              0.0              0.0  \n",
       "983               0.0              0.0              0.0  \n",
       "821               0.0              0.0              0.0  \n",
       "\n",
       "[3 rows x 6753 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = X_test[10:13]\n",
    "print(sample_test_data['rating'])\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into the first row of the **sample_test_data**. Here's the full review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I bought these for a couple of reasonsFirst I wanted an earbud that was NOT designed as noise cancelling I hate having to shove headphones into my brain to use them Apparently a lot of people are under the assumption that these are supposed to be noise cancelling or something which they are not They are simple earbuds I hate not being able to hear what is going on around me and dont feel safe in public using headphones that block everything else out These are designed just like the old style earbuds that you stick in your ears but they are more comfortable Maybe I have small ears or something but they could never fall out or dislodge from my ears They dont cancel the outside noise but they sit in my ears exactly as they are supposed to so the sound travels right into my earsSecond I really like the magnets and tangle free aspects of the headphones I use headphones while I work and the magnets allow me to hang the headphones easily when I am finished with them The flat cord and magnets also mean the headphones never really tangle when you put them in a bag or wherever Awesome designFinally the sound I get out of these seems pretty average I would compare them to the stock Apple iPhone earbuds or a set of Sony earbuds Nothing special They are only 25 and a lot of that is the fact these were just released and that you are paying for the design as far as magnets in the ear buds and the nice flat cable Dont expect them to perform like 100 headphones because they arent designed to perform like that They are just a nice simple set of earbuds that will sound good to most people The magnets and tangle free cord are awesome and they are really quite comfortableRead more'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That review seems pretty positive.\n",
    "\n",
    "Now, let's see what the next row of the **sample_test_data** looks like. As we could guess from the sentiment (-1), the review is quite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excellent product awesome color'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data['reviews'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for the **sample_test_data**. The `sentiment_model` should predict **+1** if the sentiment is positive and **-1** if the sentiment is negative. Recall from the lecture that the **score** (sometimes called **margin**) for the logistic regression model  is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{score}_i = \\mathbf{w}^T h(\\mathbf{x}_i)\n",
    "$$ \n",
    "\n",
    "where $h(\\mathbf{x}_i)$ represents the features for example $i$.  We will write some code to obtain the **scores**. For each row, the **score** (or margin) is a number in the range **[-inf, inf]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23399364 -0.03463471  0.20881002]\n"
     ]
    }
   ],
   "source": [
    "scores = sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')].to_numpy() @ weights.reshape((-1)) + sentiment_model.intercept_\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can be used to make class predictions as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w}^T h(\\mathbf{x}_i) > 0 \\\\\n",
    "      -1 & \\mathbf{w}^T h(\\mathbf{x}_i) \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Using scores, write code to calculate $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.where(scores > 0, 1, -1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions:\n",
      "[ 1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions:\")\n",
    "print(sentiment_model.predict(\n",
    "    sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your class predictions match with the one obtained from above.\n",
    "\n",
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probability predictions from the scores using:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}.\n",
    "$$\n",
    "\n",
    "Using the variable **scores** calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55823295 0.49134219 0.55201365]\n"
     ]
    }
   ],
   "source": [
    "y_prob = 1 / (1 + np.exp(-scores))\n",
    "print(y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your probability predictions match the ones obtained from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions for y=1:\n",
      "[0.55823295 0.49134219 0.55201365]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions for y=1:\")\n",
    "print(sentiment_model.predict_proba(\n",
    "    sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discussion Question:** Of the three data points in **sample_test_data**, which one (first, second, or third) has the **lowest probability** of being classified as a positive review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most positive (and negative) review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to examining the full test dataset, **test_data**, and use Numpy argsort to form predictions on all of the test data points for faster performance.\n",
    "\n",
    "Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\"\n",
    "\n",
    "To calculate these top-20 reviews, use the following steps:\n",
    "1.  Make probability predictions on **test_data** using the `sentiment_model`. (**Hint:** When you call `.predict` to make predictions on the test data.)\n",
    "2.  Sort the data according to those predictions and pick the top 20. (**Hint:** You can use indexing [-topn:] to find the top k rows sorted according to the value of a specified column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 16 18  5 11  8 26 14 24 19  7 15 25 30 23  6  9 28  2  0]\n",
      "                                                reviews  rating  \\\n",
      "512   Love the Dot Hate the Tap Portable This device...     2.0   \n",
      "217                                                 Too     2.0   \n",
      "812   Plays all my iPod music via Bluetooth Also pla...     5.0   \n",
      "234   This case is great  its inexpensive slim and v...     5.0   \n",
      "983                     Excellent product awesome color     5.0   \n",
      "968   As good as the Echo is the Echo Voice Remote s...     2.0   \n",
      "267   In this day and age of rectangles with screens...     2.0   \n",
      "134   DONT BUY a reburbished Kindle Ive had to retur...     1.0   \n",
      "10    Trs heureux que les livres soient sur Icloud A...     4.0   \n",
      "106   I previously owned an Asus infinity with 10 in...     5.0   \n",
      "213                                     very overpriced     2.0   \n",
      "389   Much improved screenIt turns out that the 4995...     5.0   \n",
      "776   hard to use and does not support Guam online i...     1.0   \n",
      "591   Im thinking this must of been a fluke it never...     2.0   \n",
      "981   Originally got the walnut cover with my Oasis ...     5.0   \n",
      "967   Ive purchased two of these screen protectors a...     5.0   \n",
      "876   Could not get it to work properly I put the ap...     1.0   \n",
      "971   Worked great for a few weeks then completely s...     1.0   \n",
      "1046  Dont bother paying for one or getting a replac...     2.0   \n",
      "1033  I dont know whats wrong with fire tv remotes b...     1.0   \n",
      "\n",
      "                                                  title  word_count_0  \\\n",
      "512                 Love the Dot, Hate the Tap Portable           0.0   \n",
      "217                                           Two Stars           0.0   \n",
      "812                                      Alexa is great           0.0   \n",
      "234                               Nice and sturdy case!           0.0   \n",
      "983   Great color One person found this helpful. Was...           0.0   \n",
      "968   Has Trouble Understanding Commands, Volume Con...           0.0   \n",
      "267                                            Just Bad           0.0   \n",
      "134                     Don't buy a refurbished Kindle!           0.0   \n",
      "10                                            Trs utile           0.0   \n",
      "106                                  Simply gorgeous 3,           0.0   \n",
      "213                                           Two Stars           0.0   \n",
      "389   IPS screen makes all the difference in the wor...           0.0   \n",
      "776                                  Not a good product           0.0   \n",
      "591                                     It never worked           0.0   \n",
      "981                            Beautiful and Functional           0.0   \n",
      "967                                  Highly recommended           0.0   \n",
      "876                            Would not work properly!           0.0   \n",
      "971                                  Dead after 3 weeks           0.0   \n",
      "1046                                      Don't Buy It.           0.0   \n",
      "1033                       Amazon fire tv remote sucks!           0.0   \n",
      "\n",
      "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "512            0.0           0.0           0.0           0.0           0.0   \n",
      "217            0.0           0.0           0.0           0.0           0.0   \n",
      "812            0.0           0.0           0.0           0.0           0.0   \n",
      "234            0.0           0.0           0.0           0.0           0.0   \n",
      "983            0.0           0.0           0.0           0.0           0.0   \n",
      "968            0.0           0.0           0.0           0.0           0.0   \n",
      "267            0.0           0.0           0.0           0.0           0.0   \n",
      "134            0.0           0.0           0.0           0.0           0.0   \n",
      "10             0.0           0.0           0.0           0.0           0.0   \n",
      "106            0.0           0.0           0.0           0.0           0.0   \n",
      "213            0.0           0.0           0.0           0.0           0.0   \n",
      "389            0.0           0.0           0.0           0.0           0.0   \n",
      "776            0.0           0.0           0.0           0.0           0.0   \n",
      "591            0.0           0.0           0.0           0.0           0.0   \n",
      "981            0.0           0.0           0.0           0.0           0.0   \n",
      "967            0.0           0.0           0.0           0.0           0.0   \n",
      "876            0.0           0.0           0.0           0.0           0.0   \n",
      "971            0.0           0.0           0.0           0.0           0.0   \n",
      "1046           0.0           0.0           0.0           0.0           0.0   \n",
      "1033           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
      "512            0.0  ...              0.0         0.000000         0.000000   \n",
      "217            0.0  ...              0.0         0.000000         0.000000   \n",
      "812            0.0  ...              0.0         0.000000         0.000000   \n",
      "234            0.0  ...              0.0         0.000000         0.000000   \n",
      "983            0.0  ...              0.0         0.000000         0.000000   \n",
      "968            0.0  ...              0.0         0.000000         0.000000   \n",
      "267            0.0  ...              0.0         0.000000         0.000000   \n",
      "134            0.0  ...              0.0         0.000000         0.000000   \n",
      "10             0.0  ...              0.0         0.000000         0.000000   \n",
      "106            0.0  ...              0.0         0.000000         0.000000   \n",
      "213            0.0  ...              0.0         0.000000         0.000000   \n",
      "389            0.0  ...              0.0         0.149304         0.076891   \n",
      "776            0.0  ...              0.0         0.000000         0.000000   \n",
      "591            0.0  ...              0.0         0.000000         0.000000   \n",
      "981            0.0  ...              0.0         0.000000         0.000000   \n",
      "967            0.0  ...              0.0         0.000000         0.000000   \n",
      "876            0.0  ...              0.0         0.000000         0.000000   \n",
      "971            0.0  ...              0.0         0.000000         0.000000   \n",
      "1046           0.0  ...              0.0         0.000000         0.000000   \n",
      "1033           0.0  ...              0.0         0.000000         0.000000   \n",
      "\n",
      "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
      "512               0.0              0.0              0.0              0.0   \n",
      "217               0.0              0.0              0.0              0.0   \n",
      "812               0.0              0.0              0.0              0.0   \n",
      "234               0.0              0.0              0.0              0.0   \n",
      "983               0.0              0.0              0.0              0.0   \n",
      "968               0.0              0.0              0.0              0.0   \n",
      "267               0.0              0.0              0.0              0.0   \n",
      "134               0.0              0.0              0.0              0.0   \n",
      "10                0.0              0.0              0.0              0.0   \n",
      "106               0.0              0.0              0.0              0.0   \n",
      "213               0.0              0.0              0.0              0.0   \n",
      "389               0.0              0.0              0.0              0.0   \n",
      "776               0.0              0.0              0.0              0.0   \n",
      "591               0.0              0.0              0.0              0.0   \n",
      "981               0.0              0.0              0.0              0.0   \n",
      "967               0.0              0.0              0.0              0.0   \n",
      "876               0.0              0.0              0.0              0.0   \n",
      "971               0.0              0.0              0.0              0.0   \n",
      "1046              0.0              0.0              0.0              0.0   \n",
      "1033              0.0              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6747  word_count_6748  word_count_6749  \n",
      "512               0.0              0.0              0.0  \n",
      "217               0.0              0.0              0.0  \n",
      "812               0.0              0.0              0.0  \n",
      "234               0.0              0.0              0.0  \n",
      "983               0.0              0.0              0.0  \n",
      "968               0.0              0.0              0.0  \n",
      "267               0.0              0.0              0.0  \n",
      "134               0.0              0.0              0.0  \n",
      "10                0.0              0.0              0.0  \n",
      "106               0.0              0.0              0.0  \n",
      "213               0.0              0.0              0.0  \n",
      "389               0.0              0.0              0.0  \n",
      "776               0.0              0.0              0.0  \n",
      "591               0.0              0.0              0.0  \n",
      "981               0.0              0.0              0.0  \n",
      "967               0.0              0.0              0.0  \n",
      "876               0.0              0.0              0.0  \n",
      "971               0.0              0.0              0.0  \n",
      "1046              0.0              0.0              0.0  \n",
      "1033              0.0              0.0              0.0  \n",
      "\n",
      "[20 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class probabilities for the test set\n",
    "y_prob = sentiment_model.predict_proba(X_test_sentiment)\n",
    "\n",
    "# Sort the test set in descending order of their probabilities of being positive\n",
    "idx = np.argsort(-y_prob[:, 1])\n",
    "\n",
    "# Get the indices of the 20 most positive reviews\n",
    "idx_most_positive = idx[-20:] \n",
    "print(idx_most_positive)\n",
    "\n",
    "# Get the corresponding reviews from the test set\n",
    "most_positive_reviews = X_test.iloc[idx_most_positive]\n",
    "print(most_positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which of the following products are represented in the 20 most positive reviews? [multiple choice]\n",
    "\n",
    "\n",
    "Now, let us repeat this exercise to find the \"most negative reviews.\" Use the prediction probabilities to find the  20 reviews in the **test_data** with the **lowest probability** of being classified as a **positive review**. Repeat the same steps above but make sure you **sort in the opposite order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                reviews  rating  \\\n",
      "989   The volume does not work with the iPhones Plea...     5.0   \n",
      "894   Great companion if you already have an echo an...     4.0   \n",
      "600   I would go for the Echo because you can talk t...     4.0   \n",
      "924   We love this speaker for listening to music th...     5.0   \n",
      "469   Just really thought it would have been more us...     2.0   \n",
      "1015  I bought these for a couple of reasonsFirst I ...     5.0   \n",
      "821   This is a great piece of hardwareI love it rea...     5.0   \n",
      "151   Okay if you dont want to read my whole review ...     5.0   \n",
      "189   Having received my Fire TV today Ive been tryi...     4.0   \n",
      "371   Really awesome tablet for my 5 year olds and w...     4.0   \n",
      "842   I was not overly impressed with this system I ...     2.0   \n",
      "512   Love the Dot Hate the Tap Portable This device...     2.0   \n",
      "217                                                 Too     2.0   \n",
      "812   Plays all my iPod music via Bluetooth Also pla...     5.0   \n",
      "234   This case is great  its inexpensive slim and v...     5.0   \n",
      "983                     Excellent product awesome color     5.0   \n",
      "968   As good as the Echo is the Echo Voice Remote s...     2.0   \n",
      "267   In this day and age of rectangles with screens...     2.0   \n",
      "134   DONT BUY a reburbished Kindle Ive had to retur...     1.0   \n",
      "10    Trs heureux que les livres soient sur Icloud A...     4.0   \n",
      "\n",
      "                                                  title  word_count_0  \\\n",
      "989   I would say these are the 'go-to' headphones f...           0.0   \n",
      "894                                             Love it           0.0   \n",
      "600                   Good sound but you have to tap it           0.0   \n",
      "924                                 Greatest thing ever           0.0   \n",
      "469                                       Not impressed           0.0   \n",
      "1015  I hate having to shove headphones into my brai...           0.0   \n",
      "821                     lovely speaker with many extras           0.0   \n",
      "151   A superb e-ink reader, but is it worth the mon...           0.0   \n",
      "189   Works great with one critical flaw. (significa...           0.0   \n",
      "371                 Good tablet, could use improvements           0.0   \n",
      "842                             I returned this product           0.0   \n",
      "512                 Love the Dot, Hate the Tap Portable           0.0   \n",
      "217                                           Two Stars           0.0   \n",
      "812                                      Alexa is great           0.0   \n",
      "234                               Nice and sturdy case!           0.0   \n",
      "983   Great color One person found this helpful. Was...           0.0   \n",
      "968   Has Trouble Understanding Commands, Volume Con...           0.0   \n",
      "267                                            Just Bad           0.0   \n",
      "134                     Don't buy a refurbished Kindle!           0.0   \n",
      "10                                            Trs utile           0.0   \n",
      "\n",
      "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "989            0.0           0.0           0.0           0.0           0.0   \n",
      "894            0.0           0.0           0.0           0.0           0.0   \n",
      "600            0.0           0.0           0.0           0.0           0.0   \n",
      "924            0.0           0.0           0.0           0.0           0.0   \n",
      "469            0.0           0.0           0.0           0.0           0.0   \n",
      "1015           0.0           0.0           0.0           0.0           0.0   \n",
      "821            0.0           0.0           0.0           0.0           0.0   \n",
      "151            0.0           0.0           0.0           0.0           0.0   \n",
      "189            0.0           0.0           0.0           0.0           0.0   \n",
      "371            0.0           0.0           0.0           0.0           0.0   \n",
      "842            0.0           0.0           0.0           0.0           0.0   \n",
      "512            0.0           0.0           0.0           0.0           0.0   \n",
      "217            0.0           0.0           0.0           0.0           0.0   \n",
      "812            0.0           0.0           0.0           0.0           0.0   \n",
      "234            0.0           0.0           0.0           0.0           0.0   \n",
      "983            0.0           0.0           0.0           0.0           0.0   \n",
      "968            0.0           0.0           0.0           0.0           0.0   \n",
      "267            0.0           0.0           0.0           0.0           0.0   \n",
      "134            0.0           0.0           0.0           0.0           0.0   \n",
      "10             0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
      "989            0.0  ...              0.0              0.0              0.0   \n",
      "894            0.0  ...              0.0              0.0              0.0   \n",
      "600            0.0  ...              0.0              0.0              0.0   \n",
      "924            0.0  ...              0.0              0.0              0.0   \n",
      "469            0.0  ...              0.0              0.0              0.0   \n",
      "1015           0.0  ...              0.0              0.0              0.0   \n",
      "821            0.0  ...              0.0              0.0              0.0   \n",
      "151            0.0  ...              0.0              0.0              0.0   \n",
      "189            0.0  ...              0.0              0.0              0.0   \n",
      "371            0.0  ...              0.0              0.0              0.0   \n",
      "842            0.0  ...              0.0              0.0              0.0   \n",
      "512            0.0  ...              0.0              0.0              0.0   \n",
      "217            0.0  ...              0.0              0.0              0.0   \n",
      "812            0.0  ...              0.0              0.0              0.0   \n",
      "234            0.0  ...              0.0              0.0              0.0   \n",
      "983            0.0  ...              0.0              0.0              0.0   \n",
      "968            0.0  ...              0.0              0.0              0.0   \n",
      "267            0.0  ...              0.0              0.0              0.0   \n",
      "134            0.0  ...              0.0              0.0              0.0   \n",
      "10             0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
      "989               0.0              0.0              0.0              0.0   \n",
      "894               0.0              0.0              0.0              0.0   \n",
      "600               0.0              0.0              0.0              0.0   \n",
      "924               0.0              0.0              0.0              0.0   \n",
      "469               0.0              0.0              0.0              0.0   \n",
      "1015              0.0              0.0              0.0              0.0   \n",
      "821               0.0              0.0              0.0              0.0   \n",
      "151               0.0              0.0              0.0              0.0   \n",
      "189               0.0              0.0              0.0              0.0   \n",
      "371               0.0              0.0              0.0              0.0   \n",
      "842               0.0              0.0              0.0              0.0   \n",
      "512               0.0              0.0              0.0              0.0   \n",
      "217               0.0              0.0              0.0              0.0   \n",
      "812               0.0              0.0              0.0              0.0   \n",
      "234               0.0              0.0              0.0              0.0   \n",
      "983               0.0              0.0              0.0              0.0   \n",
      "968               0.0              0.0              0.0              0.0   \n",
      "267               0.0              0.0              0.0              0.0   \n",
      "134               0.0              0.0              0.0              0.0   \n",
      "10                0.0              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6747  word_count_6748  word_count_6749  \n",
      "989               0.0              0.0              0.0  \n",
      "894               0.0              0.0              0.0  \n",
      "600               0.0              0.0              0.0  \n",
      "924               0.0              0.0              0.0  \n",
      "469               0.0              0.0              0.0  \n",
      "1015              0.0              0.0              0.0  \n",
      "821               0.0              0.0              0.0  \n",
      "151               0.0              0.0              0.0  \n",
      "189               0.0              0.0              0.0  \n",
      "371               0.0              0.0              0.0  \n",
      "842               0.0              0.0              0.0  \n",
      "512               0.0              0.0              0.0  \n",
      "217               0.0              0.0              0.0  \n",
      "812               0.0              0.0              0.0  \n",
      "234               0.0              0.0              0.0  \n",
      "983               0.0              0.0              0.0  \n",
      "968               0.0              0.0              0.0  \n",
      "267               0.0              0.0              0.0  \n",
      "134               0.0              0.0              0.0  \n",
      "10                0.0              0.0              0.0  \n",
      "\n",
      "[20 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the 20 most negative reviews\n",
    "idx_most_negative = idx[:20]\n",
    "\n",
    "# Get the corresponding reviews from the test set\n",
    "most_negative_reviews = X_test.iloc[idx_most_negative]\n",
    "print(most_negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question**: Which of the following products are represented in the 20 most negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "* **Step 1:** Use the trained model to compute class predictions (**Hint:** Use the `predict` method)\n",
    "* **Step 2:** Count the number of data points when the predicted class labels match the ground truth labels (called `true_labels` below).\n",
    "* **Step 3:** Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    ## YOUR CODE HERE\n",
    "    pred = model.predict(data)\n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    ## YOUR CODE HERE\n",
    "    num_correct = np.sum(pred == true_labels.to_numpy().reshape(-1))\n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    ## YOUR CODE HERE\n",
    "    accuracy = num_correct / len(data)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6774193548387096"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model, X_test_sentiment, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What is the accuracy of the **sentiment_model** on the **test_data**? Round your answer to 2 decimal places (e.g. 0.76).\n",
    "\n",
    "**Discussion Question**: Does a higher accuracy value on the **training_data** always imply that the classifier is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this assignment, we selected a 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'amazing', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(significant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words = count_vect.get_feature_names_out() # newer version of sklearn\n",
    "all_words = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, we will use the **word_count** column and trim out all words that are **not** in the **significant_words** list above. We will use the [SArray dictionary trim by keys functionality]( https://dato.com/products/create/docs/generated/graphlab.SArray.dict_trim_by_keys.html). Note that we are performing this on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_310</th>\n",
       "      <th>word_count_515</th>\n",
       "      <th>word_count_1016</th>\n",
       "      <th>word_count_1137</th>\n",
       "      <th>word_count_1838</th>\n",
       "      <th>word_count_2039</th>\n",
       "      <th>word_count_2192</th>\n",
       "      <th>word_count_2764</th>\n",
       "      <th>word_count_3522</th>\n",
       "      <th>word_count_3662</th>\n",
       "      <th>word_count_3666</th>\n",
       "      <th>word_count_3901</th>\n",
       "      <th>word_count_4164</th>\n",
       "      <th>word_count_4416</th>\n",
       "      <th>word_count_4680</th>\n",
       "      <th>word_count_5069</th>\n",
       "      <th>word_count_6489</th>\n",
       "      <th>word_count_6538</th>\n",
       "      <th>word_count_6648</th>\n",
       "      <th>word_count_6671</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_count_310  word_count_515  word_count_1016  word_count_1137  \\\n",
       "229             0.0         0.00000              0.0              0.0   \n",
       "912             0.0         0.00000              0.0              0.0   \n",
       "837             0.0         0.02871              0.0              0.0   \n",
       "728             0.0         0.00000              0.0              0.0   \n",
       "211             0.0         0.00000              0.0              0.0   \n",
       "\n",
       "     word_count_1838  word_count_2039  word_count_2192  word_count_2764  \\\n",
       "229              0.0         0.000000              0.0         0.000000   \n",
       "912              0.0         0.000000              0.0         0.000000   \n",
       "837              0.0         0.074478              0.0         0.000000   \n",
       "728              0.0         0.000000              0.0         0.418515   \n",
       "211              0.0         0.000000              0.0         0.000000   \n",
       "\n",
       "     word_count_3522  word_count_3662  word_count_3666  word_count_3901  \\\n",
       "229              0.0         0.000000              0.0         0.108244   \n",
       "912              0.0         0.181748              0.0         0.000000   \n",
       "837              0.0         0.000000              0.0         0.000000   \n",
       "728              0.0         0.268546              0.0         0.000000   \n",
       "211              0.0         0.000000              0.0         0.000000   \n",
       "\n",
       "     word_count_4164  word_count_4416  word_count_4680  word_count_5069  \\\n",
       "229              0.0         0.000000              0.0              0.0   \n",
       "912              0.0         0.123407              0.0              0.0   \n",
       "837              0.0         0.000000              0.0              0.0   \n",
       "728              0.0         0.000000              0.0              0.0   \n",
       "211              0.0         0.000000              0.0              0.0   \n",
       "\n",
       "     word_count_6489  word_count_6538  word_count_6648  word_count_6671  \n",
       "229         0.155978         0.000000              0.0          0.00000  \n",
       "912         0.000000         0.000000              0.0          0.00000  \n",
       "837         0.000000         0.022447              0.0          0.04111  \n",
       "728         0.000000         0.000000              0.0          0.00000  \n",
       "211         0.000000         0.109139              0.0          0.00000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract significant words and split train/test sets\n",
    "\n",
    "sig_word_set = set(significant_words)\n",
    "sig_idx = [i for i, e in enumerate(all_words) if e in sig_word_set]\n",
    "\n",
    "X_train_sig = X_train_sentiment.iloc[:, sig_idx]\n",
    "X_test_sig = X_test_sentiment.iloc[:, sig_idx]\n",
    "\n",
    "X_train_sig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first example of the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I typically dont post reviews however dont waste your money on this case I got it on Friday and started using it that evening Its now Thursday and the case is peeling at the edges the fabric around the camera hole has started bubbling and the felt on the inside edge is wearing off i have not had any issues with it standing on its own like other people have mentioned I am giving this a two star mainly because it is still functional I am tempted to send it back but I like the functionality of this case'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_count** column had been working with before looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_0       0.0\n",
       "word_count_1       0.0\n",
       "word_count_2       0.0\n",
       "word_count_3       0.0\n",
       "word_count_4       0.0\n",
       "                  ... \n",
       "word_count_6745    0.0\n",
       "word_count_6746    0.0\n",
       "word_count_6747    0.0\n",
       "word_count_6748    0.0\n",
       "word_count_6749    0.0\n",
       "Name: 229, Length: 6750, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sentiment.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of these words, the column **word_count_subset** is a subset of the above dictionary. In this example, only 2 `significant words` are present in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_310     0.000000\n",
       "word_count_515     0.000000\n",
       "word_count_1016    0.000000\n",
       "word_count_1137    0.000000\n",
       "word_count_1838    0.000000\n",
       "word_count_2039    0.000000\n",
       "word_count_2192    0.000000\n",
       "word_count_2764    0.000000\n",
       "word_count_3522    0.000000\n",
       "word_count_3662    0.000000\n",
       "word_count_3666    0.000000\n",
       "word_count_3901    0.108244\n",
       "word_count_4164    0.000000\n",
       "word_count_4416    0.000000\n",
       "word_count_4680    0.000000\n",
       "word_count_5069    0.000000\n",
       "word_count_6489    0.155978\n",
       "word_count_6538    0.000000\n",
       "word_count_6648    0.000000\n",
       "word_count_6671    0.000000\n",
       "Name: 229, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sig.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a classifier with **word_count_subset** as the feature and **sentiment** as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(random_state=0)\n"
     ]
    }
   ],
   "source": [
    "simple_model = LogisticRegression(random_state=0)\n",
    "simple_model.fit(X_train_sig, np.ravel(y_train))\n",
    "\n",
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5161290322580645"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model, X_test_sig, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09602371  0.33484641  0.05340021 -0.06683928 -0.19819109  0.05036369\n",
      "  -0.17518285  1.12614052 -0.09573794  0.66743953  0.         -0.22094785\n",
      "   0.22083318  0.43250797 -0.10388486 -0.16655708 -0.4217736   0.26756445\n",
      "  -0.12822802 -0.13621062]]\n"
     ]
    }
   ],
   "source": [
    "print(simple_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.12614052  0.66743953  0.43250797  0.33484641  0.26756445  0.22083318\n",
      "   0.09602371  0.05340021  0.05036369  0.         -0.06683928 -0.09573794\n",
      "  -0.10388486 -0.12822802 -0.13621062 -0.16655708 -0.17518285 -0.19819109\n",
      "  -0.22094785 -0.4217736 ]]\n"
     ]
    }
   ],
   "source": [
    "print(-np.sort(-simple_model.coef_, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Consider the coefficients of **simple_model**. There should be 21 of them, an intercept term + one for each word in **significant_words**. How many of the 20 coefficients (corresponding to the 20 **significant_words** and *excluding the intercept term*) are positive for the `simple_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "num_positive_weights = np.sum(simple_model.coef_ >= 0)\n",
    "print(num_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "First, compute the classification accuracy of the **sentiment_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917355371900827\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_train_acc = get_classification_accuracy(sentiment_model, X_train_sentiment, y_train)\n",
    "print(sentiment_model_train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the classification accuracy of the **simple_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6446280991735537\n"
     ]
    }
   ],
   "source": [
    "simple_model_test_acc = get_classification_accuracy(simple_model, X_train_sig, y_train)\n",
    "print(simple_model_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6774193548387096\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_test_acc = get_classification_accuracy(sentiment_model, X_test_sentiment, y_test)\n",
    "print(sentiment_model_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5161290322580645"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_test_acc = get_classification_accuracy(simple_model, X_test_sig, y_test)\n",
    "simple_model_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Comment out the sention on 'Match number of positive and negative reviews' and re-run the notebook. Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "What is the majority class in the **train_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "num_positive = int(np.sum(y_train == +1))\n",
    "num_negative = int(np.sum(y_train == -1))\n",
    "print(num_positive)\n",
    "print(num_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of the majority class classifier on **test_data**.\n",
    "\n",
    "**Discussion Question**: Enter the accuracy of the majority class classifier model on the **test_data**. Round your answer to two decimal places (e.g. 0.76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment    0.451613\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if num_positive >= num_negative:\n",
    "    y_pred = 1\n",
    "else:\n",
    "    y_pred = -1\n",
    "\n",
    "n_correct = np.sum(y_test == y_pred)\n",
    "accuracy = n_correct / len(X_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Is the **sentiment_model** definitely better than the majority class classifier (the baseline)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression via Stochastic Gradient Descent\n",
    "\n",
    "The goal of this notebook is to implement a logistic regression classifier using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1,  1, -1,\n",
       "        1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1, -1,  1, -1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Define Hyperparams\n",
    "max_iter=1000           # number of passes on training data\n",
    "tol=1e-3                # stopping criteria for iterations\n",
    "penalty='l2'            # 'l1' and 'l2' regularization term\n",
    "alpha=0.001             # Constant that multiplies the regularization term. Ranges from [0 Inf)\n",
    "loss='log'              # 'log' is logistic regression, 'hinge' for Support Vector Machine\n",
    "random_state=42         # seed of the pseudo random number generated which is used while shuffling the data\n",
    "\n",
    "# Make pipeline\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=max_iter, \n",
    "                                  tol=tol,\n",
    "                                  penalty=penalty,\n",
    "                                  alpha=alpha,\n",
    "                                  loss=loss,\n",
    "                                  random_state=random_state))\n",
    "clf.fit(X_train_sentiment, np.ravel(y_train))\n",
    "\n",
    "sentiment_predictions = clf.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "sentiment_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent with Cross Validation\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define Hyperparams\n",
    "max_iter=1000           # number of passes on training data\n",
    "tol=1e-3                # stopping criteria for iterations\n",
    "penalty='l2'            # 'l1' and 'l2' regularization term\n",
    "alpha=0.001             # Constant that multiplies the regularization term. Ranges from [0 Inf)\n",
    "loss='log'              # 'log' is logistic regression, 'hinge' for Support Vector Machine\n",
    "random_state=42         # seed of the pseudo random number generated which is used while shuffling the data\n",
    "\n",
    "# evaluation method\n",
    "n_splits=4\n",
    "n_repeats=5\n",
    "sgd_cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "\n",
    "# define model\n",
    "ridge_model = SGDClassifier(max_iter=max_iter, \n",
    "                              tol=tol,\n",
    "                              penalty=penalty,\n",
    "                              alpha=alpha,\n",
    "                              loss=loss,\n",
    "                              random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.744 (0.091)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ridge_model, X_train_sentiment, np.ravel(y_train), scoring='accuracy',cv=sgd_cv, n_jobs=-1)\n",
    "\n",
    "# force scores to be positive\n",
    "scores = abs(scores)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The accuracy, while convenient, does not tell the whole story. For a fuller picture, we turn to the **confusion matrix**. In the case of binary classification, the confusion matrix is a 2-by-2 matrix laying out correct and incorrect predictions made in each label as follows:\n",
    "```\n",
    "              +---------------------------------------------+\n",
    "              |                Predicted label              |\n",
    "              +----------------------+----------------------+\n",
    "              |          (-1)        |         (+1)         |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "| True  |(-1) | # of true negative  | # of false positive |\n",
    "| label +-----+----------------------+----------------------+\n",
    "|       |(+1) | # of false negative | # of true positive  |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "```\n",
    "To print out the confusion matrix for a classifier, use `metric='confusion_matrix'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  4],\n",
       "       [ 6, 11]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sentiment_predictions = sentiment_model.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "cmatrix = confusion_matrix(y_test, sentiment_predictions)\n",
    "cmatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: How many predicted values in the **test set** are **false positives**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 false positives\n",
      "There are 6 false negatives\n",
      "There are 11 true positives\n",
      "There are 10 true negatives\n"
     ]
    }
   ],
   "source": [
    "# [Ans]\n",
    "true_neg, false_pos, false_neg, true_pos = cmatrix.ravel()\n",
    "print('There are {} false positives'.format(false_pos))\n",
    "print('There are {} false negatives'.format(false_neg))\n",
    "print('There are {} true positives'.format(true_pos))\n",
    "print('There are {} true negatives'.format(true_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "\n",
    "Put yourself in the shoes of a manufacturer that sells a product on Amazon.com and you want to monitor your product's reviews in order to respond to complaints.  Even a few negative reviews may generate a lot of bad publicity about the product. So you don't want to miss any reviews with negative sentiments --- you'd rather put up with false alarms about potentially negative reviews instead of missing negative reviews entirely. In other words, **false positives cost more than false negatives**. (It may be the other way around for other scenarios, but let's stick with the manufacturer's scenario for now.)\n",
    "\n",
    "Suppose you know the costs involved in each kind of mistake: \n",
    "1. \\$100 for each false positive.\n",
    "2. \\$1 for each false negative.\n",
    "3. Correctly classified reviews incur no cost.\n",
    "\n",
    "**Quiz Question**: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the **test set**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives costs $400, false negatives cost $6 with a total cost of $406\n"
     ]
    }
   ],
   "source": [
    "fp_cost = false_pos*100\n",
    "fn_cost = false_neg*1\n",
    "print('False positives costs ${}, false negatives cost ${} with a total cost of ${}'.format(fp_cost,fn_cost,fp_cost+fn_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to reduce the percentage of false positives to be less than, say, 3.5% of all positive predictions. This is where **precision** comes in:\n",
    "\n",
    "$$\n",
    "[\\text{precision}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all data points with positive predictions]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false positives}]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to keep the percentage of false positives below 3.5% of positive predictions, we must raise the precision to 96.5% or higher. \n",
    "\n",
    "**First**, let us compute the precision of the logistic regression classifier on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on test data: 0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "precision = true_pos/(true_pos+false_pos)\n",
    "print(\"Precision on test data: %s\" % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Out of all reviews in the **test set** that are predicted to be positive, what fraction of them are **false positives**? (Round to the second decimal place e.g. 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pos / (true_pos + false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Based on what we learned in lecture, if we wanted to reduce this fraction of false positives to be below 3.5%, we would (select one):\n",
    "\n",
    "- Discard a sufficient number of positive predictions\n",
    "- Discard a sufficient number of negative predictins\n",
    "- Increase threshold for predicting the positive class ($y_{hat} = +1$)\n",
    "- Decrease threshold for predicting the positive class ($y_{hat} = +1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complementary metric is **recall**, which measures the ratio between the number of true positives and that of (ground-truth) positive reviews:\n",
    "\n",
    "$$\n",
    "[\\text{recall}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all positive data points]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false negatives}]}\n",
    "$$\n",
    "\n",
    "Let us compute the recall on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on test data: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "recall = true_pos / (true_pos + false_neg)\n",
    "print(\"Recall on test data: %s\" % recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What fraction of the positive reviews in the **test_set** were correctly predicted as positive by the classifier?\n",
    "\n",
    "**Discussion Question**: What is the recall value for a classifier that predicts **+1** for all data points in the **test_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos / (true_pos + false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-recall tradeoff\n",
    "\n",
    "In this part, we will explore the trade-off between precision and recall discussed in the lecture.  We first examine what happens when we use a different threshold value for making class predictions.  We then explore a range of threshold values and plot the associated precision-recall curve.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the threshold\n",
    "\n",
    "False positives are costly in our example, so we may want to be more conservative about making positive predictions. To achieve this, instead of thresholding class probabilities at 0.5, we can choose a higher threshold. \n",
    "\n",
    "Write a function called `apply_threshold` that accepts two things\n",
    "* `probabilities` (an SArray of probability values)\n",
    "* `threshold` (a float between 0 and 1).\n",
    "\n",
    "The function should return an SArray, where each element is set to +1 or -1 depending whether the corresponding probability exceeds `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(probabilities, threshold):\n",
    "    ### YOUR CODE GOES HERE\n",
    "    # +1 if >= threshold and -1 otherwise.\n",
    "    return np.array([1 if p[1] >= threshold else -1 for p in probabilities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction with `output_type='probability'` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = sentiment_model.predict_proba(X_test_sentiment)\n",
    "predictions_with_default_threshold = apply_threshold(probabilities, 0.5)\n",
    "predictions_with_high_threshold = apply_threshold(probabilities, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predicted reviews (threshold = 0.5): 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive predicted reviews (threshold = 0.5): %s\" % (predictions_with_default_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predicted reviews (threshold = 0.9): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive predicted reviews (threshold = 0.9): %s\" % (predictions_with_high_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the associated precision and recall as the threshold varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold = 0.5\n",
    "precision_with_default_threshold = precision_score(\n",
    "    y_test, predictions_with_default_threshold)\n",
    "\n",
    "recall_with_default_threshold = recall_score(\n",
    "    y_test, predictions_with_default_threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Threshold = 0.9\n",
    "precision_with_high_threshold = precision_score(\n",
    "    y_test, predictions_with_high_threshold, zero_division=1)\n",
    "\n",
    "recall_with_high_threshold = recall_score(\n",
    "    y_test, predictions_with_high_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (threshold = 0.5): 0.7333333333333333\n",
      "Recall (threshold = 0.5)   : 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision (threshold = 0.5): %s\" % precision_with_default_threshold)\n",
    "print(\"Recall (threshold = 0.5)   : %s\" % recall_with_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (threshold = 0.9): 1.0\n",
      "Recall (threshold = 0.9)   : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision (threshold = 0.9): %s\" % precision_with_high_threshold)\n",
    "print(\"Recall (threshold = 0.9)   : %s\" % recall_with_high_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question (variant 1)**: Does the **precision** increase with a higher threshold?\n",
    "\n",
    "**Discussion Question (variant 2)**: Does the **recall** increase with a higher threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "\n",
    "Now, we will explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the values of threshold, we compute the precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "probabilities = sentiment_model.predict_proba(X_test_sentiment)\n",
    "for threshold in threshold_values:\n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "\n",
    "    precision = precision_score(y_test, predictions, zero_division=1)\n",
    "\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    \n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the precision-recall curve to visualize the precision-recall tradeoff as we vary the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pr_curve(precision, recall, title):\n",
    "    plt.rcParams['figure.figsize'] = 7, 5\n",
    "    plt.locator_params(axis = 'x', nbins = 5)\n",
    "    plt.plot(precision, recall, 'b-', linewidth=4.0, color = '#B0017F')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "plot_pr_curve(precision_all, recall_all, 'Precision recall curve (all)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5606060606060606\n"
     ]
    }
   ],
   "source": [
    "for idx, precision in enumerate(precision_all):\n",
    "    if precision >= 0.965:\n",
    "        print(threshold_values[idx])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? (**Hint**: You may use the Sklearn precision and recall functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = apply_threshold(probabilities, 0.98)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of false negatives (i.e the number of reviews to look at when not needed) that we have to deal with using this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating specific search terms\n",
    "\n",
    "So far, we looked at the number of false positives for the **entire test set**. In this section, let's select reviews using a specific search term and optimize the precision on these reviews only. After all, a manufacturer would be interested in tuning the false positive rate just for their products (the reviews they want to read) rather than that of the entire set of products on Amazon.\n",
    "\n",
    "## Precision-Recall on all tv related items\n",
    "\n",
    "From the **test set**, select all the reviews for all products with the word 'tv' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_reviews = X_test_sentiment[X_test['reviews'].apply(lambda x: 'tv' in x.lower())]\n",
    "tv_y = y_test[X_test['reviews'].apply(lambda x: 'tv' in x.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict the probability of classifying these reviews as positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = sentiment_model.predict_proba(tv_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the precision-recall curve for the **tv_reviews** dataset.\n",
    "\n",
    "**First**, let's consider the following `threshold_values` ranging from 0.5 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, as we did above, let's compute precision and recall for each value in `threshold_values` on the **tv_reviews** dataset.  Complete the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "for threshold in threshold_values:    \n",
    "    # Make predictions. Use the `apply_threshold` function \n",
    "    ## YOUR CODE HERE \n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "\n",
    "    # Calculate the precision.\n",
    "    # YOUR CODE HERE\n",
    "    precision = precision_score(tv_y, predictions, zero_division=1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    recall = recall_score(tv_y, predictions)\n",
    "    \n",
    "    # Append the precision and recall scores.\n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better for the reviews of data in **products**? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "for idx, precision in enumerate(precision_all):\n",
    "    if precision >= 0.965:\n",
    "        print(threshold_values[idx])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 96.5%?\n",
    "\n",
    "**Finally**, let's plot the precision recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pr_curve(precision_all, recall_all, \"Precision-Recall (tv)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the precision vs. recall values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "85463d1bb2888979eba71184c8d462df281b59dd3aecce6d9ffe8745b11b3329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
